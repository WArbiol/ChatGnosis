# ü§ñ Chat Gnosis: Uma Aplica√ß√£o de RAG com LLMs

[![Python 3.9+](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.35-red.svg)](https://streamlit.io)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.2-orange.svg)](https://pytorch.org/)
[![Licen√ßa: MIT](https://img.shields.io/badge/Licen√ßa-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Uma aplica√ß√£o inteligente de Q&A que permite conversar com uma grande cole√ß√£o de documentos PDF, utilizando um pipeline de **Retrieval-Augmented Generation (RAG)** para fornecer respostas fi√©is, r√°pidas e com fontes citadas.

---

## üé• Demonstra√ß√£o

![Placeholder para o GIF da aplica√ß√£o](https://i.imgur.com/your-gif-placeholder.gif)

---

## üí° Sobre o Projeto

Este projeto busca responder as perguntas do usu√°rio com base em um banco de dados alimentado por livros Gn√≥stico, possui uma aplica√ß√£o web completa, modular e pronta para deploy, demonstrando pr√°ticas modernas de engenharia de software para sistemas de IA. O objetivo √© criar uma ferramenta de pesquisa sem√¢ntica robusta, onde um usu√°rio pode fazer perguntas complexas em linguagem natural e receber respostas sintetizadas a partir de uma base de conhecimento privada de dezenas (ou centenas) de documentos.

A arquitetura foi cuidadosamente planejada para separar as responsabilidades, garantindo que a aplica√ß√£o seja escal√°vel, de f√°cil manuten√ß√£o e flex√≠vel para futuras expans√µes.

## üöÄ Tecnologias e Arquitetura

A aplica√ß√£o √© constru√≠da sobre um stack tecnol√≥gico moderno e eficiente, escolhido para otimizar tanto a performance quanto a experi√™ncia de desenvolvimento.

### **Frontend**

- **Streamlit:** Utilizado para a cria√ß√£o de uma interface de usu√°rio reativa, bonita e interativa com poucas linhas de c√≥digo Python. A escolha ideal para prototipagem r√°pida e desenvolvimento de aplica√ß√µes de dados.

### **Backend & L√≥gica de IA (Pipeline RAG)**

O cora√ß√£o da aplica√ß√£o √© um pipeline de Retrieval-Augmented Generation (RAG) customizado, dividido em duas fases principais:

#### 1. **Indexa√ß√£o (A Mem√≥ria da IA)**

Este processo offline transforma os documentos PDF em uma base de conhecimento vetorial otimizada para buscas r√°pidas.

- **Extra√ß√£o de Texto:** `PyMuPDF` foi escolhido por sua alta performance e efici√™ncia na extra√ß√£o de texto bruto de PDFs.
- **Fragmenta√ß√£o Sem√¢ntica:** `langchain-text-splitters` √© utilizado para quebrar os textos em "chunks" de forma inteligente, preservando o contexto ao evitar quebras no meio de senten√ßas ou par√°grafos.
- **Vetoriza√ß√£o (Embeddings):** `sentence-transformers` √© usado para carregar e executar modelos de embedding de √∫ltima gera√ß√£o (`BAAI/bge-m3`). Esta etapa converte os chunks de texto em vetores num√©ricos que capturam seu significado sem√¢ntico.
- **Banco de Dados Vetorial:** `ChromaDB` atua como nosso banco de dados vetorial local e persistente, armazenando os embeddings e metadata para permitir buscas por similaridade em milissegundos.

#### 2. **Recupera√ß√£o e Gera√ß√£o (O Racioc√≠nio da IA)**

Este processo online √© ativado a cada pergunta do usu√°rio.

- **Motor de Gera√ß√£o (LLM):** O motor √© desacoplado, permitindo o uso de LLMs locais via **Ollama** (como `Qwen2`, `Llama 3`, `Phi-3`) para desenvolvimento e privacidade, ou a integra√ß√£o com APIs de nuvem (**Google Gemini API**) para um deploy escal√°vel e de baixo custo. Esta flexibilidade arquitetural √© um dos pontos fortes do projeto.
- **Prompt Engineering:** Uma estrat√©gia de prompt robusta √© utilizada para instruir o LLM a basear suas respostas estritamente no contexto recuperado, citar as fontes e evitar "alucina√ß√µes", garantindo a fidelidade da informa√ß√£o.

## ‚ú® Funcionalidades

- [x] **Interface Reativa e Moderna:** Tema escuro customiz√°vel com CSS.
- [x] **Chat em Tempo Real:** Respostas geradas por streaming, com efeito de "digita√ß√£o".
- [x] **Fidelidade e Cita√ß√£o de Fontes:** A IA exibe os trechos exatos dos documentos originais que usou para formular a resposta.
- [x] **Pipeline de Ingest√£o Desacoplado:** Um script dedicado para processar e indexar os documentos, separando o "modo admin" do "modo usu√°rio".
- [x] **Suporte a GPU:** O c√≥digo √© capaz de detectar e utilizar GPUs NVIDIA para uma acelera√ß√£o massiva no processamento dos modelos, tanto para embeddings quanto para a gera√ß√£o de texto.
- [x] **Pronto para Deploy:** Arquitetura projetada com deploy em mente, separando o c√≥digo-fonte dos artefatos de dados.

## üîß Instala√ß√£o e Execu√ß√£o

Para executar este projeto localmente, siga os passos abaixo:

**1. Pr√©-requisitos:**

- Python 3.9 ou superior
- Git
- (Opcional, para rodar LLM local) [Ollama](https://ollama.com/) instalado.
- (Opcional, para acelera√ß√£o) GPU NVIDIA com drivers e CUDA Toolkit configurados.

**2. Clone o Reposit√≥rio:**

```bash
git clone [https://github.com/SEU_USUARIO/NOME_DO_SEU_REPOSITORIO.git](https://github.com/SEU_USUARIO/NOME_DO_SEU_REPOSITORIO.git)
cd NOME_DO_SEU_REPOSITORIO
```

**3. Crie e Ative um Ambiente Virtual:**

```bash
python -m venv venv
source venv/bin/activate  # No macOS/Linux
# ou
.\venv\Scripts\activate  # No Windows
```

**4. Instale as Depend√™ncias:**

```bash
pip install -r requirements.txt
```

_(Lembre-se: para suporte a GPU, instale a vers√£o do PyTorch com CUDA, conforme o site oficial)._

**5. Configure suas Chaves de API (se aplic√°vel):**

- Se for usar a API do Google Gemini, crie o arquivo `.streamlit/secrets.toml` e adicione sua chave:
  ```toml
  GOOGLE_API_KEY = "SUA_CHAVE_API_AQUI"
  ```

**6. Prepare a Base de Conhecimento:**

- Coloque seus arquivos PDF na pasta `data/pdfs/`.
- Execute o script de ingest√£o uma √∫nica vez:
  ```bash
  python scripts/build_vector_store.py
  ```

**7. Execute a Aplica√ß√£o:**

- (Se usar Ollama) Certifique-se de que o Ollama est√° rodando com o modelo desejado (ex: `ollama run qwen2`).
- Inicie a aplica√ß√£o Streamlit:
  ```bash
  streamlit run app.py
  ```

## ‚òÅÔ∏è Preparado para Deploy

Esta aplica√ß√£o foi projetada para ser facilmente implantada usando padr√µes modernos:

- **Frontend:** Pode ser implantado gratuitamente no **Streamlit Community Cloud**.
- **Base de Conhecimento:** A pasta `vector_store` gerada pode ser hospedada em um servi√ßo de armazenamento de artefatos como o **Hugging Face Hub**, mantendo o reposit√≥rio Git leve.
- **LLM Backend:** A arquitetura suporta a troca do Ollama por uma chamada de API (como a do Gemini), permitindo um deploy "serverless" de baixo custo e alta escalabilidade.

## üë®‚Äçüíª Autor

**Walter Melhado Arbiol Forn√©**

- **LinkedIn:** [linkedin.com/in/walter-melhado-arbiol-forne-818656211/](https://www.linkedin.com/in/walter-melhado-arbiol-forne-818656211/)
- **GitHub:** [github.com/WArbiol](https://https://github.com/WArbiol)
- **Email:** [walterarbiol@gmail.com](mailto:walterarbiol@gmail.com)

## üìú Licen√ßa

Distribu√≠do sob a licen√ßa MIT. Veja `LICENSE` para mais informa√ß√µes.
